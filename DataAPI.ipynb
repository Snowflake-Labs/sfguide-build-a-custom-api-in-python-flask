{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c54d9db-d9ed-4391-a31c-74240c6314e4",
   "metadata": {
    "collapsed": false,
    "name": "Introduction"
   },
   "source": [
    "# Build a Custom API in Python and Flask\n",
    "\n",
    "## 1. Overview \n",
    "\n",
    "Many builders want to share some of the data stored in Snowflake over an http API. Modern mobile and web applications often want to retrieve that data through http APIs. This tutorial will go through how to build, deploy, and host a custom API Powered by Snowflake.\n",
    "\n",
    "This API consists of reporting endpoints from data stored in Snowflake. After completing this guide, you will have built a custom API built with [Python Flask](https://flask.palletsprojects.com/). \n",
    "\n",
    "The dataset is the [TPC-H](https://docs.snowflake.com/en/user-guide/sample-data-tpch) data set included in your Snowflake account.\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "- Privileges necessary to create a user, database, warehouse, compute pool, repository, network rule, external access integration, and service in Snowflake\n",
    "- Privileges necessary to access the tables in the `SNOWFLAKE_SAMPLE_DATA.TPCH_SF10` database and schema\n",
    "- Access to run SQL in the Snowflake console or SnowSQL\n",
    "- Basic experience using git, GitHub, and Codespaces\n",
    "- Intermediate knowledge of Python\n",
    "\n",
    "### What You’ll Learn \n",
    "- How to configure and build a custom API Powered by Snowflake\n",
    "- How to build, publish, and deploy a container with the API in Snowflake\n",
    "\n",
    "### What You’ll Need \n",
    "- [Snowflake](https://snowflake.com) Account in an AWS commercial region\n",
    "- [GitHub](https://github.com/) Account with credits for Codespaces\n",
    "\n",
    "### What You’ll Build \n",
    "- API Powered by Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400f7b4-39e0-41fd-90db-ac92d914e15e",
   "metadata": {
    "collapsed": false,
    "name": "Setting_up_your_Development_Environment"
   },
   "source": [
    "---\n",
    "## 2. Setting up your Development Environment\n",
    "\n",
    "The code used in this guide is hosted in github. You will need a new Codespace from the GitHub [repository](https://github.com/Snowflake-Labs/sfguide-build-a-custom-api-in-python-flask).\n",
    "\n",
    "To create a new codespace, browse to the GitHub [repository](https://github.com/Snowflake-Labs/sfguide-build-a-custom-api-in-python-flask) in a browser. \n",
    "You will need to login to GitHub if you are not already logged in to access Codespaces. After logging in, click on the green \"<> Code\" button and \"create codespace on main\" button.\n",
    "\n",
    "You will then be redirected into Codespaces where your development environment will load and all code from GitHub will be loaded in the project. \n",
    "\n",
    "Let's take a quick look at the code in this repository.\n",
    "\n",
    "### Endpoints\n",
    "The API creates two sets of endpoints, one for using the Snowflake connector:\n",
    "1. `https://host/connector/customers/top10`\n",
    "  * Which takes the following optional query parameters:\n",
    "    1. `start_range` - the start date of the range in `YYYY-MM-DD` format. Defaults to `1995-01-01`.\n",
    "    1. `end_range` - the end date of the range in `YYYY-MM-DD` format. Defaults to `1995-03-31`.\n",
    "2. `https://host/connector/clerk/CLERK_ID/yearly_sales/YEAR`\n",
    "  * Which takes 2 required path parameters:\n",
    "    1. `CLERK_ID` - the clerk ID. Use just the numbers, such as `000000001`.\n",
    "    2. `YEAR` - the year to use, such as `1995`.\n",
    "\n",
    "And the same ones using Snowpark:\n",
    "1. `https://host/snowpark/customers/top10`\n",
    "  * Which takes the following optional query parameters:\n",
    "    1. `start_range` - the start date of the range in `YYYY-MM-DD` format. Defaults to `1995-01-01`.\n",
    "    1. `end_range` - the end date of the range in `YYYY-MM-DD` format. Defaults to `1995-03-31`.\n",
    "2. `https://host/snowpark/clerk/CLERK_ID/yearly_sales/YEAR`\n",
    "  * Which takes 2 required path parameters:\n",
    "    1. `CLERK_ID` - the clerk ID. Use just the numbers, such as `000000001`.\n",
    "    2. `YEAR` - the year to use, such as `1995`.\n",
    "\n",
    "### Code\n",
    "The `src/` directory has all the source code for the API. The `connector.py` file contains all the entrypoints for the API endpoints using the Snowflake Connector for Python. \n",
    "The `customers_top10()` function is one of the API endpoints we needed for this API which finds the top 10 customers by sales in a date range. \n",
    "Review the code and the SQL needed to retrieve the data from Snowflake and serialize it to JSON for the response. \n",
    "This endpoint also takes 2 optional query string parameters start_range and end_range.\n",
    "\n",
    "```python\n",
    "@connector.route('/customers/top10')\n",
    "def customers_top10():\n",
    "    # Validate arguments\n",
    "    sdt_str = request.args.get('start_range') or '1995-01-01'\n",
    "    edt_str = request.args.get('end_range') or '1995-03-31'\n",
    "    try:\n",
    "        sdt = datetime.datetime.strptime(sdt_str, dateformat)\n",
    "        edt = datetime.datetime.strptime(edt_str, dateformat)\n",
    "    except:\n",
    "        abort(400, \"Invalid start and/or end dates.\")\n",
    "    sql_string = '''\n",
    "        SELECT\n",
    "            o_custkey\n",
    "          , SUM(o_totalprice) AS sum_totalprice\n",
    "        FROM snowflake_sample_data.tpch_sf10.orders\n",
    "        WHERE o_orderdate >= '{sdt}'\n",
    "          AND o_orderdate <= '{edt}'\n",
    "        GROUP BY o_custkey\n",
    "        ORDER BY sum_totalprice DESC\n",
    "        LIMIT 10\n",
    "    '''\n",
    "    sql = sql_string.format(sdt=sdt, edt=edt)\n",
    "    try:\n",
    "        res = conn.cursor(DictCursor).execute(sql)\n",
    "        return make_response(jsonify(res.fetchall()))\n",
    "    except:\n",
    "        abort(500, \"Error reading from Snowflake. Check the logs for details.\")\n",
    "```\n",
    "\n",
    "You can also review the other endpoints in [connector.py](https://github.com/Snowflake-Labs/sfguide-build-a-custom-api-in-python-flask/blob/main/src/connector.py) to \n",
    "see how simple it is to host multiple endpoints.\n",
    "\n",
    "If you would also like to see how to build endpoints using the Snowflake Snowpark API, \n",
    "review [snowpark.py](https://github.com/Snowflake-Labs/sfguide-build-a-custom-api-in-python-flask/blob/main/src/snowpark.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201326a-2cd2-4869-bebb-acfed3a92fb0",
   "metadata": {
    "collapsed": false,
    "name": "Setting_up_Snowflake_CLI"
   },
   "source": [
    "---\n",
    "## 3. Setting up Snowflake CLI\n",
    "\n",
    "All of the commands in this step will be run in the terminal in Codespaces.\n",
    "\n",
    "First, we need to install Snowflake CLI, with the following command in the terminal:\n",
    "```bash\n",
    "pip install snowflake-cli\n",
    "```\n",
    "\n",
    "Next, we will create a connection for SnowCLI to our Snowflake account. When you\n",
    "run the following command in a terminal, you will be prompted for various details. \n",
    "You only need to supply a connection name (use `my_snowflake`), the user name, and\n",
    "the password. The other prompts are optional:\n",
    "\n",
    "```bash\n",
    "snow connection add\n",
    "```\n",
    "\n",
    "To make this connection the default connection that SnowCLI uses, run the following in the terminal:\n",
    "\n",
    "```bash\n",
    "snow connection set-default my_snowflake\n",
    "```\n",
    "\n",
    "Test that the connection is properly set up by running the following in a terminal:\n",
    "\n",
    "```bash\n",
    "snow connection test\n",
    "```\n",
    "\n",
    "Let's create a database for this lab using Snowflake CLI. Run the following command in a terminal:\n",
    "\n",
    "```bash\n",
    "snow object create database name=api\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0074de90-d006-49b0-b5ea-87945611aa82",
   "metadata": {
    "collapsed": false,
    "name": "Creating_a_Notebook"
   },
   "source": [
    "---\n",
    "## 4. Creating a Notebook for this Lab\n",
    "\n",
    "It is useful to use a Notebook to follow the steps for this lab. It allows multiple commands to be put in a single cell and executed, and it allows\n",
    "seeing the output of previous steps.\n",
    "\n",
    "You can create a new Notebook in Snowflake and copy commands from this guide into new cells and execute them. Alternatively, the repo for this\n",
    "lab comes with a Notebook file you can use to create a Notebook in Snowflake with all of the commands in it.\n",
    "\n",
    "### Importing the Notebook file\n",
    "To create a Notebook with this lab and commands in it, first download the `DataAPI.ipynb` file from the lab repository, \n",
    "[here](https://github.com/Snowflake-Labs/sfguide-build-a-custom-api-in-python-flask/blob/main/DataAPI.ipynb). \n",
    "If you are using Codespaces, you can right-click on the file in the file explorer and choose \"Download\".\n",
    "\n",
    "Next, in the Snowflake console, choose the \"Projects\" sidebar and select \"Notebooks\". Choose the down arrow next to the \"+ Notebook\"\n",
    "button and select \"Import .ipynb file\". You will be prompted to choose the file from your machine - choose the `DataAPI.ipynb` file that you saved.\n",
    "Next, you will be shown a form to collect information about your Notebook. You can choose any name you would like (e.g., `Data API`). \n",
    "Choose the `API` database and the `PUBLIC` schema. Choose \"Run on warehouse\". Leave all of the other inputs with their defaults.\n",
    "\n",
    "When the Notebook is created, click the \"Start\" button on the top.\n",
    "\n",
    "### Creating an empty Notebook\n",
    "To create an empty Notebook, in the Snowflake console, choose the \"Projects\" sidebar and select \"Notebooks\". Click the \"+ Notebook\" button\n",
    "in the top left. Next, you will be shown a form to collect information about your Notebook. You can choose any name you would like (e.g., `Data API`). \n",
    "Choose the `API` database and the `PUBLIC` schema. Choose \"Run on warehouse\". Leave all of the other inputs with their defaults.\n",
    "\n",
    "When the Notebook is created, click the \"Start\" button on the top.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51e81d-bbb7-4226-9841-58aef1715129",
   "metadata": {
    "collapsed": false,
    "name": "Setting_up_a_Database_and_Warehouse"
   },
   "source": [
    "---\n",
    "## 5. Setting up a Database and Warehouse\n",
    "\n",
    "The API needs a warehouse to query the data to return to the caller. To create the database and warehouse, \n",
    "run the following commands in the Snowflake (in a cell in a Snowflake Notebook, in a Worksheet in the Snowflake console, or using SnowSQL):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a155280-358b-4bd4-abea-4158abd6fadc",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "USE ROLE ACCOUNTADMIN;\n",
    "CREATE DATABASE IF NOT EXISTS API;\n",
    "CREATE WAREHOUSE IF NOT EXISTS DATA_API_WH WITH WAREHOUSE_SIZE='xsmall';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d7c8d-91b0-4af7-8530-0172b6c5028c",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "### Create the Application Role in Snowflake\n",
    "\n",
    "The application will run as a new role with minimal priviledges. The following commands create the role and grant it access to the data needed for the application.\n",
    "Run the following commands in the Snowflake (in a cell in a Snowflake Notebook, in a Worksheet in the Snowflake console, or using SnowSQL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81068d08-40db-4236-b0c4-c356c7d9f539",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "USE ROLE ACCOUNTADMIN;\n",
    "CREATE ROLE IF NOT EXISTS DATA_API_ROLE;\n",
    "\n",
    "GRANT ALL ON DATABASE API TO ROLE DATA_API_ROLE;\n",
    "GRANT ALL ON SCHEMA API.PUBLIC TO ROLE DATA_API_ROLE;\n",
    "GRANT USAGE ON WAREHOUSE DATA_API_WH TO ROLE DATA_API_ROLE;\n",
    "GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE_SAMPLE_DATA TO ROLE DATA_API_ROLE;\n",
    "\n",
    "GRANT ROLE DATA_API_ROLE TO ROLE ACCOUNTADMIN;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7482539-6534-4f41-b270-6c5f3d29cac0",
   "metadata": {
    "collapsed": false,
    "name": "Creating_the_Image_Repository"
   },
   "source": [
    "---\n",
    "## 6. Creating the Image Repository\n",
    "\n",
    "To create the image repository, run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a6df7d-10d4-4820-a910-36057362dc77",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "USE ROLE ACCOUNTADMIN;\n",
    "USE DATABASE API;\n",
    "CREATE OR REPLACE IMAGE REPOSITORY API;\n",
    "\n",
    "GRANT READ ON IMAGE REPOSITORY API TO ROLE DATA_API_ROLE;\n",
    "\n",
    "SHOW IMAGE REPOSITORIES;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22545624-ab59-4074-9068-5c6d15bf1383",
   "metadata": {
    "collapsed": false,
    "name": "cell7"
   },
   "source": [
    "Note the `repository_url` in the response as that will be needed in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f2dc7b-da85-47a1-9268-0e5b7fd431cf",
   "metadata": {
    "collapsed": false,
    "name": "Building_the_Application_Container"
   },
   "source": [
    "---\n",
    "## 7. Building the Application Container\n",
    "\n",
    "The commands in this step are to be run in a terminal in Codespaces.\n",
    "\n",
    "To create the application container, we will leverage docker. The Dockerfile is based on python 3.8 and installs the required libraries needed for \n",
    "the application as well as the code. To create the docker container, run this command in the terminal provided by Codespaces:\n",
    "\n",
    "```bash\n",
    "docker build -t dataapi .\n",
    "```\n",
    "\n",
    "Next, we need to tag the Docker image and push it to the image repository. To do so, replace the `<repository_url>` with the `repository_url` value \n",
    "returned by the `SHOW IMAGE REPOSITORIES` command you ran above.\n",
    "\n",
    "```bash\n",
    "docker tag dataapi <repository_url>/dataapi\n",
    "```\n",
    "\n",
    "Lastly, we need to push the image to Snowflake. Before we do that, we need to log into the Image Registry for Docker. To do so, run:\n",
    "\n",
    "```bash\n",
    "snow spcs image-registry login\n",
    "```\n",
    "\n",
    "And finally we can push it to the image repository. \n",
    "```bash\n",
    "docker push <repository_url>/dataapi\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b7f3e-f66b-42bb-b652-74d3b84a79a8",
   "metadata": {
    "collapsed": false,
    "name": "Creating_the_Compute_Pool"
   },
   "source": [
    "---\n",
    "## 8. Creating the Compute Pool\n",
    "\n",
    "To create the compute pool to run the application, run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb93830a-973c-4953-9cd5-d520967a79d2",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "CREATE COMPUTE POOL API\n",
    "  MIN_NODES = 1\n",
    "  MAX_NODES = 5\n",
    "  INSTANCE_FAMILY = CPU_X64_XS;\n",
    "\n",
    "GRANT USAGE ON COMPUTE POOL API TO ROLE DATA_API_ROLE;\n",
    "GRANT MONITOR ON COMPUTE POOL API TO ROLE DATA_API_ROLE;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43408d54-2aad-474c-8f7e-e46abccdb6bc",
   "metadata": {
    "collapsed": false,
    "name": "cell11"
   },
   "source": [
    "You can see the status of the `API` compute pool by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc43b1-a07f-4957-89c0-bd7974c4f4e7",
   "metadata": {
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "SHOW COMPUTE POOLS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c56a249-e3c3-4e3f-a487-9682660c9e9a",
   "metadata": {
    "collapsed": false,
    "name": "Creating_the_Application_Service"
   },
   "source": [
    "---\n",
    "## 9. Creating the Application Service\n",
    "\n",
    "To create the service to host the application, run the following commands in the Snowflake (in a cell in a Snowflake Notebook, in a Worksheet in the Snowflake console, or using SnowSQL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d3045-e4b3-453d-a92e-566e413d5d86",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "USE ROLE ACCOUNTADMIN;\n",
    "GRANT BIND SERVICE ENDPOINT ON ACCOUNT TO ROLE DATA_API_ROLE;\n",
    "\n",
    "USE ROLE DATA_API_ROLE;\n",
    "CREATE SERVICE API.PUBLIC.API\n",
    " IN COMPUTE POOL API\n",
    " FROM SPECIFICATION  \n",
    "$$\n",
    "spec:\n",
    "  container:\n",
    "  - name: api\n",
    "    image: /api/public/api/dataapi:latest\n",
    "    resources:                          \n",
    "      requests:\n",
    "        cpu: 0.5\n",
    "        memory: 128M\n",
    "      limits:\n",
    "        cpu: 1\n",
    "        memory: 256M\n",
    "  endpoint:\n",
    "  - name: api\n",
    "    port: 8001\n",
    "    public: true\n",
    "serviceRoles:\n",
    "- name: api_sr\n",
    "  endpoints:\n",
    "  - api\n",
    "$$\n",
    "QUERY_WAREHOUSE = DATA_API_WH;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7c70ae-31f5-410c-bb49-85ba1d22caf2",
   "metadata": {
    "collapsed": false,
    "name": "cell15"
   },
   "source": [
    "It will take a few minutes for your service to initialize, you can check status with these commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb4a5f1-1125-419e-899d-0c1faac27536",
   "metadata": {
    "language": "sql",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "SHOW SERVICES IN COMPUTE POOL API;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12fe55-b67f-4a0b-a6da-da3b76607a86",
   "metadata": {
    "language": "sql",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "CALL SYSTEM$GET_SERVICE_STATUS('api.public.api');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e3d1c8-4d8d-45b3-8a48-06fb43d44df1",
   "metadata": {
    "language": "sql",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "CALL SYSTEM$GET_SERVICE_LOGS('api.public.api', 0, 'api');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb4c179-4a52-4005-ac48-4effe02bb28c",
   "metadata": {
    "collapsed": false,
    "name": "cell19"
   },
   "source": [
    "After your service has started, you can get the endpoints with the following command. Note that provisioning endpoints\n",
    "can take a moment. While it does you will get a note like `Endpoints provisioning in progress...`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730ce74-bab4-48c1-8b2d-c671401c81a5",
   "metadata": {
    "language": "sql",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "SHOW ENDPOINTS IN SERVICE API.PUBLIC.API;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6ed33-e09b-4271-94e8-b12edd6c7a8f",
   "metadata": {
    "collapsed": false,
    "name": "cell21"
   },
   "source": [
    "Make note of the `ingress_url` as that will be need to test the application. This service will start the API, running at `https://<INGRESS_URL>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88b4de-2635-4486-9b08-dbcba29357a8",
   "metadata": {
    "collapsed": false,
    "name": "Testing_the_API"
   },
   "source": [
    "---\n",
    "## 10. Testing the API\n",
    "\n",
    "To verify the API is online, go to the `https://<INGRESS_URL>` in your browser. You will be asked to authenticate to Snowflake and be given the root content: \n",
    "\n",
    "```json\n",
    "{\"result\":\"Nothing to see here\"}\n",
    "```\n",
    "\n",
    "### Endpoints\n",
    "This API was implemented using both the Snowflake Python Connector and the Snowflake Snowpark for Python package. \n",
    "They both implement the same API routes. The ones implemented with the Snowflke Python Connector are under the `/connector/` route.\n",
    "The ones implemented with Snowpark Python are under the `/snowpark/` route.\n",
    "\n",
    "#### Top 10 Customers\n",
    "To retrieve the top 10 customers in the date range of `1995-02-01` to `1995-02-14` using the Snowflake Connector for Python, use:\n",
    "\n",
    "```\n",
    "https://<INGRESS_URL>/connector/customers/top10?start_range=1995-02-01&end_range=1995-02-14\n",
    "```\n",
    "\n",
    "To retrieve the top 10 customers in the date range of `1995-02-01` to `1995-02-14` using the Snowflake Snowpark API, use:\n",
    "```\n",
    "https://<INGRESS_URL>/snowpark/customers/top10?start_range=1995-02-01&end_range=1995-02-14\n",
    "```\n",
    "\n",
    "If you call the endpoint without specifying the `start_range` then `1995-01-01` will be used. If you call the endpoint without specifying the `end_range` then `1995-03-31` will be used.\n",
    "\n",
    "#### Monthly sales for a given year for a sales clerk\n",
    "To retrieve the monthly sales for clerk `000000002` for the year `1995` using the Snowflake Connector for Python, run:\n",
    "```\n",
    "https://<INGRESS_URL>/connector/clerk/000000002/yearly_sales/1995\n",
    "```\n",
    "\n",
    "To retrieve the monthly sales for clerk `000000002` for the year `1995` using the Snowflake Snowpark API, run:\n",
    "```\n",
    "https://<INGRESS_URL>/snowpark/clerk/000000002/yearly_sales/1995\n",
    "```\n",
    "\n",
    "### Testing using a webpage\n",
    "This project comes with a simple webpage that allows you to test the API. To get to it, open `https://<INGRESS_URL>/test` in a web browser.\n",
    "\n",
    "At the top you can choose if you want to exercise the Snowflake Connector for Python or the Snowflake Snowpark API endpoints.\n",
    "\n",
    "There are 2 forms below that. The first one allows you to enter parameters to test the \"Top 10 Customers\" endpoint. \n",
    "The second one allows you to enter parameters to test the \"Monthly Clerk Sales\" endpoint.\n",
    "\n",
    "When you hit the `Submit` button, the API endpoint is called and the data is returned to the web page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5585b2e0-48e1-440e-b2f7-aea1a75856aa",
   "metadata": {
    "collapsed": false,
    "name": "Programmatic_Access"
   },
   "source": [
    "---\n",
    "## 11. Programmatic Access\n",
    "\n",
    "In many situations we want to access this data API from another process outside of Snowflake programmatically. To do this, we will need a way to programmatically \n",
    "authenticate to Snowflake to allow access to the SPCS endpoint. There are a number of ways to do this today, but we will focus on using \n",
    "Programmatic Access Tokens (PAT), one of the simpler ways. \n",
    "\n",
    "\n",
    "Regardless of the authenictation method, the best practice is to create a user specifically for accessing this API endpoint, as well as a role for that user. \n",
    "To do this, run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f64aaa-f2e6-4d1f-a5ef-fd3e7a420c96",
   "metadata": {
    "language": "sql",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "USE ROLE ACCOUNTADMIN;\n",
    "CREATE ROLE IF NOT EXISTS APIROLE;\n",
    "GRANT ROLE APIROLE TO ROLE ACCOUNTADMIN;\n",
    "GRANT USAGE ON DATABASE API TO ROLE APIROLE;\n",
    "GRANT USAGE ON SCHEMA API.PUBLIC TO ROLE APIROLE;\n",
    "CREATE USER IF NOT EXISTS APIUSER PASSWORD='User123' DEFAULT_ROLE = apirole DEFAULT_SECONDARY_ROLES = ('ALL') MUST_CHANGE_PASSWORD = FALSE;\n",
    "GRANT ROLE APIROLE TO USER APIUSER;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5dd008-99e6-4933-8132-9cc9893b6947",
   "metadata": {
    "collapsed": false,
    "name": "cell25"
   },
   "source": [
    "Next, we can grant the service role to access the endpoint to the APIROLE we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f080085-2a0b-42ef-b6e4-21f29340cc7b",
   "metadata": {
    "language": "sql",
    "name": "cell26"
   },
   "outputs": [],
   "source": [
    "USE ROLE ACCOUNTADMIN;\n",
    "GRANT SERVICE ROLE api.public.api!api_sr TO ROLE apirole;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e025219-1354-4232-a358-cf1cfb161e70",
   "metadata": {
    "collapsed": false,
    "name": "cell27"
   },
   "source": [
    "### Generating a PAT token via SQL\n",
    "Lastly, we need to create a Programmatic Access Token for the `APIUSER`.\n",
    "In order to use PAT, the user must have a network policy applied, so we create an \"allow-all\" network policy for this user. In practice you would limit to the\n",
    "IP/hostname origins for your clients.\n",
    "\n",
    "You can do this via SQL as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796136e-19e3-4125-a418-318cd1560e8c",
   "metadata": {
    "language": "sql",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "USE ROLE ACCOUNTADMIN;\n",
    "CREATE NETWORK POLICY IF NOT EXISTS api_np ALLOWED_IP_LIST = ('0.0.0.0/0');\n",
    "ALTER USER apiuser SET NETWORK_POLICY = api_np;\n",
    "ALTER USER IF EXISTS apiuser ADD PROGRAMMATIC ACCESS TOKEN api_token;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac46cf-2163-490c-b47e-f9c9cc8f420b",
   "metadata": {
    "collapsed": false,
    "name": "cell29"
   },
   "source": [
    "Copy this PAT token and save it to a file. Save it to a file named `apiuser-token-secret.txt` in the `test/` directory of the cloned/downloaded repo in Codespaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c56365-3c1f-4e7d-9aff-e55a7e72aac7",
   "metadata": {
    "collapsed": false,
    "name": "cell30"
   },
   "source": [
    "### Generating a PAT token via Snowsight\n",
    "Alternatively, you can use Snowsight to create the PAT token. Click on the \"Admin\" option on the sidebar, then the \"Users & Roles\" option in the sidebar.\n",
    "Next, click on the APIUSER user, and scroll down to the \"Programmatic access tokens\" section. Click the \"Generate new token\" button, give the token a name\n",
    "(such as `api_token`), choose the role `APIROLE` from the pull-down, leave the rest of the defaults, and click \"Generate\". Click the \"Download token\" button\n",
    "and save the file to the `test/` directory (you can leave the default filename)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5fe5a-7b98-474b-bb07-602e4b02e821",
   "metadata": {
    "collapsed": false,
    "name": "Testing_Programmatically"
   },
   "source": [
    "---\n",
    "## 12. Testing Programmatically\n",
    "\n",
    "Next we can test accessing our API programmatically using the PAT token. We cannot use the PAT token directly to access the SPCS endpoint.\n",
    "We must exchange the long-lived PAT token for a shorter-lived access token via the `/oauth/token` Snowflake endpoint. We can then use that\n",
    "shorter-lived token to access the SPCS endpoint. \n",
    "\n",
    "We have 2 applications that demonstrate how to do this.\n",
    "\n",
    "### Accessing the endpoint via command-line program\n",
    "Change to the `test/` directory. In there is a program named `test.py`. You can see the usage instructions by running:\n",
    "\n",
    "```bash\n",
    "python test.py --help\n",
    "```\n",
    "\n",
    "You must supply the following:\n",
    "* `ACCOUNT_URL` - this is the URL for your Snowflake account. It should be of the form `<ORGNAME>-<ACCTNAME>.snowflakecomputing.com`. You can find this in the\n",
    "  Snowflake console by clicking the circle with initials in the lower left and choosing the \"Connect a tool to Snowflake\" menu option. Copy the field\n",
    "  named \"Account/Server URL\".\n",
    "* `ROLE` - the role to use when accessing the endpoint. For this example, it should be `APIROLE`.\n",
    "* `ENDPOINT` - this is the full URL you are trying to access. E.g., `https://<INGRESS_URL>/connector/customers/top10`\n",
    "\n",
    "There are 3 ways to specify the PAT to use:\n",
    "1. Use the `--pat` option and supply the full PAT token. E.g., `--pat <PAT>`.\n",
    "2. Use the `--patfile` option and supply the filename to the PAT token file. E.g., `--patfile /path/to/patfile`\n",
    "3. If you saved your PAT token to this directory, and it ends with `-token-secret.txt` the application will discover it and use it.\n",
    "\n",
    "For example, your call might look something like:\n",
    "\n",
    "```bash\n",
    "python test.py --account_url 'MYORG-MYACCT.snowflakecomputing.com' --role APIROLE --endpoint 'https://mzbqa5c-myorg-myacct.snowflakecomputing.app/connector/customers/top10'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82ad4f-44f3-4e26-99a5-4333e44f804f",
   "metadata": {
    "collapsed": false,
    "name": "cell32"
   },
   "source": [
    "### Accessing the endpoint via Streamlit\n",
    "The repository also contains a Streamlit to access the endpoint. \n",
    "\n",
    "To use Streamlit we must first install the Streamlit library:\n",
    "\n",
    "```bash\n",
    "pip install streamlit\n",
    "```\n",
    "\n",
    "Next, change to the `test/` directory, and run:\n",
    "\n",
    "```bash\n",
    "python -m streamlit run test_streamlit.py\n",
    "```\n",
    "\n",
    "Enter the account URL, role, and URL in the supplied boxes.\n",
    "\n",
    "The Streamlit will attempt to detect the PAT in the local directory in a file ending with `-token-secret.txt`. If one is found, it will use that as the PAT. \n",
    "If not, it will show another box to enter the PAT in (the actual PAT value, not the filename).\n",
    "\n",
    "Enter the necessary items and click \"Fetch it!\". You will get a status update that it is \"Trading PAT for Token...\" and then \"Getting data...\" and then it will\n",
    "display the result from SPCS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cff554-68ad-4c3b-81f7-b0d962e36ac1",
   "metadata": {
    "collapsed": false,
    "name": "Stopping_the_API"
   },
   "source": [
    "---\n",
    "## 13. Stopping the API\n",
    "\n",
    "To stop the API, you can suspend the service. \n",
    "Run the following commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e0747-85a0-4997-91fa-a4e5606b6a93",
   "metadata": {
    "language": "sql",
    "name": "cell34"
   },
   "outputs": [],
   "source": [
    "USE ROLE DATA_API_ROLE;\n",
    "ALTER SERVICE API.PUBLIC.API SUSPEND;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e5997-08a9-4e9f-97e2-1ffb23c3ea37",
   "metadata": {
    "collapsed": false,
    "name": "Cleanup"
   },
   "source": [
    "## 14. Cleanup\n",
    "\n",
    "To fully remove everything you did today you only need to drop some objects in your Snowflake account. \n",
    "Run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eeff63-7922-4c78-b30b-183a310e2c52",
   "metadata": {
    "language": "sql",
    "name": "cell36"
   },
   "outputs": [],
   "source": [
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "DROP DATABASE IF EXISTS API;\n",
    "DROP ROLE IF EXISTS DATA_API_ROLE;\n",
    "DROP COMPUTE POOL IF EXISTS API;\n",
    "DROP WAREHOUSE IF EXISTS DATA_API_WH;\n",
    "DROP USER IF EXISTS APIUSER;\n",
    "DROP ROLE IF EXISTS APIROLE;\n",
    "DROP NETWORK POLICY api_np;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252b041-a2de-4bfb-b0e1-f8ec6bb67158",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "You can now turn off your Codespaces environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130ca6b-f043-4cc3-bc8c-5c28f7cd7931",
   "metadata": {
    "collapsed": false,
    "name": "Conclusion"
   },
   "source": [
    "## 15. Conclusion and Resources\n",
    "### Overview\n",
    "You've successfully built a custom API in Python Powered by Snowflake. \n",
    "\n",
    "When you go to put a data API into production you should protect the API with some level of authentication and authorization. \n",
    "\n",
    "Another consideration is enabling a frontend website to access the endpoint, the test site worked in this example because it's hosted on the same domain as the api. If you need to access the api from another website, you will need to do additional configuration.\n",
    "\n",
    "To get more comfortable with this solution, implement new endpoints pointing to the sample dataset provided or other datasets.\n",
    "\n",
    "### What You Learned\n",
    "- How to configure and build a custom API Powered by Snowflake\n",
    "- How to run and test the API on your machine\n",
    "\n",
    "### Resources\n",
    "Code for this project is available at [https://github.com/sfc-gh-bhess/lab_data_api_python](https://github.com/sfc-gh-bhess/lab_data_api_python).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "",
   "authorId": "2448766293310",
   "authorName": "USER",
   "lastEditTime": 1748555745199,
   "notebookId": "73pvtveipegysnr7heta",
   "sessionId": "c6aa1af1-db0f-4ffb-9b09-c659efff1f81"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
